{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm 2 - Assignment 5\n",
    "\n",
    "### Main objective\n",
    "\n",
    "Learn the structure of the Bayesian Network (BN) resulting from the dataset (https://archive.ics.uci.edu/dataset/19/car+evaluation) using two BN structure learning algorithms of your choice.  For instance you can consider the algorithms implemented in PGMPY or any other comparable library (e.g. see the list of libraries listed in Lecture 7).  Compare and discuss the results obtained with the two different algorithms. Also discuss any hyperparameter/design choice you had to take.\n",
    "\n",
    "### Choices made\n",
    "\n",
    "For this assignment I chose the PC algorithm and the Hill Climbing algorithm to learn the structure of the Bayesian Network. The PC algorithm is a constraint-based algorithm that uses conditional independence tests to learn the structure of the network. The Hill Climbing algorithm is a score-based algorithm that uses a scoring function to evaluate the quality of the network structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "From the dataset I applied One-hot encoding to the categorical features, since we have more than 2 categories for each feature. I also split the dataset into training and testing sets, with 80% of the data used for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  buying  maint doors persons lug_boot safety  class\n",
      "0  vhigh  vhigh     2       2    small    low  unacc\n",
      "1  vhigh  vhigh     2       2    small    med  unacc\n",
      "2  vhigh  vhigh     2       2    small   high  unacc\n",
      "3  vhigh  vhigh     2       2      med    low  unacc\n",
      "4  vhigh  vhigh     2       2      med    med  unacc\n",
      "buying      0\n",
      "maint       0\n",
      "doors       0\n",
      "persons     0\n",
      "lug_boot    0\n",
      "safety      0\n",
      "class       0\n",
      "dtype: int64\n",
      "TR data\n",
      "\n",
      "\n",
      "TS data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\"\n",
    "names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
    "data = pd.read_csv(url, names=names)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Preprocessing\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Assuming 'data' is the original dataset containing features and target variable\n",
    "# Extract features (X) and target variable (y)\n",
    "X = data.drop(columns=['class'])  # Features\n",
    "y = data['class']  # Target variable\n",
    "\n",
    "# One-hot encode the features and target variable, with 0 and 1 like sklearn does\n",
    "# to use them, uncomment the lines that use np.append and comment the lines that use pd.concat\n",
    "\n",
    "#X_encoded = OneHotEncoder().fit_transform(X).toarray()\n",
    "#y_encoded = OneHotEncoder().fit_transform(y.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Data encoded with pandas, uncomment the following lines and the next commented lines that use pd.concat\n",
    "X_encoded = pd.get_dummies(X)\n",
    "y_encoded = pd.get_dummies(y)\n",
    "\n",
    "# Concatenate the features and target variable\n",
    "#data_encoded = np.append(X_encoded, y_encoded, axis=1)\n",
    "data_encoded = pd.concat([X_encoded, y_encoded], axis=1)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Unencoded data\n",
    "\n",
    "# Encoded data sets for training and testing\n",
    "data_train_encoded = np.append(X_train, y_train, axis=1)\n",
    "data_test_encoded = np.append(X_test, y_test, axis=1)\n",
    "\n",
    "# Unencoded data sets for training and testing\n",
    "#data_train_unencoded = pd.concat([X_train, y_train], axis=1)\n",
    "#data_test_unencoded = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "data_train_encoded = pd.concat([X_train, y_train], axis=1)\n",
    "data_test_encoded = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "print(\"TR data\")\n",
    "print(data_train_encoded)\n",
    "print(\"\\n\")\n",
    "print(\"TS data\")\n",
    "print(data_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PC algorithm\n",
    "\n",
    "In this section I have instantiate a Bayesian Network with the method of the library pgmpy and used the PC algorithm to learn the structure of the network. Then, I added the nodes and edges to the model.\n",
    "\n",
    "Here, we can observe that the algorithm takes quite a while to complete, which is expected since the PC algorithm is known to be computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working for n conditional variables: 0:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working for n conditional variables: 5: 100%|██████████| 5/5 [00:03<00:00,  1.77it/s]INFO:pgmpy:Reached maximum number of allowed conditional variables. Exiting\n",
      "Working for n conditional variables: 5: 100%|██████████| 5/5 [00:03<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes and edges of the PC model:\n",
      "['lug_boot', 'class', 'persons', 'buying', 'safety', 'maint']\n",
      "[('lug_boot', 'class'), ('persons', 'class'), ('buying', 'class'), ('safety', 'class'), ('maint', 'class')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import PC\n",
    "from pgmpy.models import BayesianNetwork\n",
    "\n",
    "# Initialize the PC algorithm with data and a different significance level\n",
    "model_pc = BayesianNetwork()\n",
    "pc = PC(pd.DataFrame(data_train_encoded))\n",
    "\n",
    "# Constraint-based structure learning, with PC algorithm\n",
    "dag = pc.estimate(return_type='dag', variant='parallel', significance_level=0.01)\n",
    "\n",
    "model_pc.add_nodes_from(dag.nodes())    \n",
    "model_pc.add_edges_from(dag.edges())\n",
    "\n",
    "# Display the edges of the learned structure\n",
    "print(\"Nodes and edges of the PC model:\")\n",
    "print(model_pc.nodes())\n",
    "print(model_pc.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hill Climbing Algorithm\n",
    "\n",
    "In this section I have instantiate a Bayesian Network with the method of the library pgmpy and used the Hill Climbing algorithm to learn the structure of the network. Then, I added the nodes and edges to the model, as I did with the PC algorithm.\n",
    "\n",
    "The Hill Climbing algorithm is also computationally expensive, but it is faster than the PC algorithm. We can clearly see that the HC algorithm takes less than a second to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/1000000 [00:00<1:28:59, 187.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes and edges of the HC model:\n",
      "['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
      "[('maint', 'buying'), ('doors', 'buying'), ('persons', 'buying'), ('lug_boot', 'buying'), ('safety', 'persons'), ('safety', 'buying'), ('safety', 'lug_boot'), ('class', 'safety'), ('class', 'persons'), ('class', 'buying'), ('class', 'maint'), ('class', 'lug_boot')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Learning with hill climbing\n",
    "from pgmpy.estimators import HillClimbSearch\n",
    "\n",
    "# Initialize the Hill Climbing search\n",
    "model_hc = BayesianNetwork()\n",
    "hc = HillClimbSearch(pd.DataFrame(data_train_encoded))\n",
    "\n",
    "# Structure learning with Hill Climbing search algorithm\n",
    "best_model = hc.estimate(scoring_method='k2score')\n",
    "\n",
    "model_hc.add_nodes_from(best_model.nodes())\n",
    "model_hc.add_edges_from(best_model.edges())\n",
    "print(\"Nodes and edges of the HC model:\")\n",
    "print(model_hc.nodes())\n",
    "print(model_hc.edges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Little sidenote\n",
    "\n",
    "I also tried and inserted the Max-min Hill Climbing search but it was taking too long to complete (when data is one-hot encoded), so I decided to remove it from the final version of the code. I left it commented in the code for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1000000 [00:00<29:49, 558.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes and edges of the MMHC model:\n",
      "['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
      "[('class', 'safety'), ('class', 'persons'), ('class', 'buying'), ('class', 'maint'), ('class', 'lug_boot')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# MmHC algorithm\n",
    "from pgmpy.estimators import MmhcEstimator\n",
    "\n",
    "# Initialize the MmHC algorithm\n",
    "model_mmhc = BayesianNetwork()\n",
    "mmhc = MmhcEstimator(pd.DataFrame(data_train_encoded))\n",
    "\n",
    "# Structure learning with MmHC algorithm\n",
    "best_model = mmhc.estimate(scoring_method='k2score', significance_level=0.01)\n",
    "\n",
    "model_mmhc.add_nodes_from(best_model.nodes())\n",
    "model_mmhc.add_edges_from(best_model.edges())\n",
    "print(\"Nodes and edges of the MMHC model:\")\n",
    "print(model_mmhc.nodes())\n",
    "print(model_mmhc.edges())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data fitting\n",
    "\n",
    "After learning the structure with the 2 methods, I fitted the training data into the models using the Maximum Likelihood Estimation (MLE) method for both of them.\n",
    "\n",
    "Other estimator can be used, such as the Bayesian Estimator, but for this assignment I chose the MLE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning with Maximum Likelihood Estimation, other methods are available\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "\n",
    "# Estimate parameters using Maximum Likelihood Estimation (MLE) for PC model\n",
    "model_pc.fit(pd.DataFrame(data_train_encoded), estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "# Estimate parameters using Maximum Likelihood Estimation (MLE) for HC model\n",
    "model_hc.fit(pd.DataFrame(data_train_encoded), estimator=MaximumLikelihoodEstimator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions \n",
    "\n",
    "After fitting the data into the models, I made predictions using the testing data. I used the predict() method to predict the values of the target variable given the values of the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data has variables which are not in the model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Predictions using PC model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m y_pred_pc \u001b[38;5;241m=\u001b[39m model_pc\u001b[38;5;241m.\u001b[39mpredict(pd\u001b[38;5;241m.\u001b[39mDataFrame(X_test))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions using PC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_pred_pc)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pgmpy/models/BayesianNetwork.py:696\u001b[0m, in \u001b[0;36mBayesianNetwork.predict\u001b[0;34m(self, data, stochastic, n_jobs)\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo variable missing in data. Nothing to predict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(data\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes()):\n\u001b[0;32m--> 696\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData has variables which are not in the model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    698\u001b[0m missing_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(data\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    699\u001b[0m model_inference \u001b[38;5;241m=\u001b[39m VariableElimination(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Data has variables which are not in the model"
     ]
    }
   ],
   "source": [
    "# Predictions using PC model\n",
    "y_pred_pc = model_pc.predict(pd.DataFrame(X_test))\n",
    "\n",
    "print(\"Predictions using PC\")\n",
    "print(y_pred_pc)\n",
    "\n",
    "# Predictions using HC model\n",
    "y_pred_hc = model_hc.predict(pd.DataFrame(X_test))\n",
    "\n",
    "print(\"Predictions using HC\")\n",
    "print(y_pred_hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes in PC model: 25\n",
      "Total edges in PC model: 30\n",
      "\n",
      "\n",
      "Total nodes in HC model: 25\n",
      "Total edges in HC model: 55\n"
     ]
    }
   ],
   "source": [
    "# Get the number of nodes (variables) and edges in PC model\n",
    "print(\"Total nodes in PC model:\", len(model_pc.nodes()))\n",
    "print(\"Total edges in PC model:\", len(model_pc.edges()))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get the number of nodes (variables) and edges in HC model\n",
    "print(\"Total nodes in HC model:\", len(model_hc.nodes()))\n",
    "print(\"Total edges in HC model:\", len(model_hc.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final scores\n",
    "\n",
    "After training and testing, I calculated 2 kinds of scoring methods used in the pgmpy library: structure and correlation scores. In particular, these scores are used to evaluate the quality of the structure of the Bayesian Network.\n",
    "\n",
    "In scores.py I implemented the 2 methods that print all the possible structure and correlation scores for the models, in order to better evaluate the quality of the structure of the Bayesian Networks and compare them.\n",
    "\n",
    "### Structure scores of the models\n",
    "\n",
    "The structure scores are metrics that are used to evaluate the quality of the structure of the Bayesian Network. The higher the score, the better the structure of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure score for PC:\n",
      "k2 score: -3384.9104972256455\n",
      "bdeu score: -3424.808248803261\n",
      "bic score: -3375.2576757455663\n",
      "bds score: -3782.005628508058\n",
      "\n",
      "\n",
      "Structure score for HC:\n",
      "k2 score: -2957.889126228843\n",
      "bdeu score: -2958.594353603475\n",
      "bic score: -3152.0380245178853\n",
      "bds score: -3494.292343403773\n"
     ]
    }
   ],
   "source": [
    "from scores import compute_structure_score\n",
    "\n",
    "print(\"Structure scores for PC:\")\n",
    "compute_structure_score(model_pc, data_test_encoded)\n",
    "print(\"\\n\")\n",
    "print(\"Structure scores for HC:\")\n",
    "compute_structure_score(model_hc, data_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation scores of the models\n",
    "\n",
    "Correlation scores of the Bayesian models are used to evaluate the quality of the model. The higher the score, the better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation score for PC:\n",
      "chi_square score (accuracy): 0.9314516129032258\n",
      "g_sq score (accuracy): 0.9249492900608519\n",
      "log_likelihood score (accuracy): 0.9249492900608519\n",
      "freeman_tuckey score (accuracy): 0.9224489795918367\n",
      "modified_log_likelihood score (accuracy): 0.9202453987730062\n",
      "neyman score (accuracy): 0.9202453987730062\n",
      "cressie_read score (accuracy): 0.9292929292929293\n",
      "\n",
      "\n",
      "Correlation score for HC:\n",
      "chi_square score (accuracy): 0.5482866043613707\n",
      "g_sq score (accuracy): 0.5534591194968553\n",
      "log_likelihood score (accuracy): 0.5534591194968553\n",
      "freeman_tuckey score (accuracy): 0.5523809523809524\n",
      "modified_log_likelihood score (accuracy): 0.554140127388535\n",
      "neyman score (accuracy): 0.554140127388535\n",
      "cressie_read score (accuracy): 0.55\n"
     ]
    }
   ],
   "source": [
    "from scores import compute_correlation_score\n",
    "print(\"Correlation scores for PC:\")\n",
    "compute_correlation_score(model_pc, data_test_encoded)\n",
    "print(\"\\n\")\n",
    "print(\"Correlation scores for HC:\")\n",
    "compute_correlation_score(model_hc, data_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ispr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
