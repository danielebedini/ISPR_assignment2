{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm 2 - Assignment 5\n",
    "\n",
    "### Main objective\n",
    "\n",
    "Learn the structure of the Bayesian Network (BN) resulting from the dataset DSET4 using two BN structure learning algorithms of your choice.  For instance you can consider the algorithms implemented in PGMPY or any other comparable library (e.g. see the list of libraries listed in Lecture 7).  Compare and discuss the results obtained with the two different algorithms. Also discuss any hyperparameter/design choice you had to take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  buying  maint doors persons lug_boot safety  class\n",
      "0  vhigh  vhigh     2       2    small    low  unacc\n",
      "1  vhigh  vhigh     2       2    small    med  unacc\n",
      "2  vhigh  vhigh     2       2    small   high  unacc\n",
      "3  vhigh  vhigh     2       2      med    low  unacc\n",
      "4  vhigh  vhigh     2       2      med    med  unacc\n",
      "TR data\n",
      "[[0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 1. 0.]\n",
      " [0. 1. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n",
      "\n",
      "\n",
      "TS data\n",
      "[[1. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\"\n",
    "names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
    "data = pd.read_csv(url, names=names)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Preprocessing\n",
    "# Check for missing values\n",
    "# print(data.isnull().sum())\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming 'data' is the original dataset containing features and target variable\n",
    "# Extract features (X) and target variable (y)\n",
    "X = data.drop(columns=['class'])  # Features\n",
    "y = data['class']  # Target variable\n",
    "\n",
    "# One-hot encode the features and target variable\n",
    "X_encoded = OneHotEncoder().fit_transform(X).toarray()\n",
    "y_encoded = OneHotEncoder().fit_transform(y.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Concatenate the features and target variable\n",
    "data_encoded = np.append(X_encoded, y_encoded, axis=1)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "data_train_encoded = np.append(X_train, y_train, axis=1)\n",
    "data_test_encoded = np.append(X_test, y_test, axis=1)\n",
    "\n",
    "print(\"TR data\")\n",
    "print(data_train_encoded)\n",
    "print(\"\\n\")\n",
    "print(\"TS data\")\n",
    "print(data_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PC algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielebedini/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Working for n conditional variables: 5: 100%|██████████| 5/5 [00:04<00:00,  1.01s/it]INFO:pgmpy:Reached maximum number of allowed conditional variables. Exiting\n",
      "Working for n conditional variables: 5: 100%|██████████| 5/5 [00:04<00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes and edges of the PC model:\n",
      "[5, 4, 21, 22, 18, 19, 20, 14, 12, 6, 7, 24, 23, 2, 0, 3, 13, 16, 17, 1, 8, 9, 15, 11, 10]\n",
      "[(5, 4), (5, 7), (21, 22), (21, 23), (18, 19), (18, 24), (18, 20), (20, 24), (20, 19), (14, 12), (14, 13), (12, 23), (6, 4), (6, 7), (6, 5), (2, 0), (2, 3), (13, 12), (16, 17), (1, 0), (1, 3), (1, 2), (8, 9), (8, 10), (8, 11), (15, 17), (15, 16), (11, 9), (11, 10), (10, 9)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import PC\n",
    "from pgmpy.models import BayesianNetwork\n",
    "\n",
    "# Initialize the PC algorithm with data and a different significance level\n",
    "model_pc = BayesianNetwork()\n",
    "pc = PC(pd.DataFrame(data_train_encoded))\n",
    "\n",
    "# Constraint-based structure learning, with PC algorithm\n",
    "dag = pc.estimate(return_type='dag')\n",
    "\n",
    "model_pc.add_nodes_from(dag.nodes())    \n",
    "model_pc.add_edges_from(dag.edges())\n",
    "\n",
    "# Display the edges of the learned structure\n",
    "print(\"Nodes and edges of the PC model:\")\n",
    "print(model_pc.nodes())\n",
    "print(model_pc.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hill Climbing Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 56/1000000 [00:00<4:37:15, 60.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes and edges of the HC model:\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "[(0, 2), (0, 1), (0, 12), (1, 2), (3, 2), (3, 1), (3, 0), (3, 12), (4, 3), (5, 4), (5, 6), (6, 4), (7, 4), (7, 6), (7, 3), (7, 5), (9, 8), (9, 10), (9, 11), (10, 8), (11, 8), (11, 10), (12, 13), (12, 14), (14, 13), (15, 17), (16, 17), (16, 15), (17, 21), (18, 19), (18, 20), (18, 21), (19, 21), (19, 12), (19, 22), (19, 7), (20, 19), (21, 23), (21, 3), (21, 7), (22, 23), (22, 5), (22, 21), (22, 0), (22, 6), (22, 1), (23, 12), (23, 3), (23, 7), (24, 23), (24, 18), (24, 21), (24, 15), (24, 0), (24, 5)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Learning with hill climbing\n",
    "from pgmpy.estimators import HillClimbSearch\n",
    "\n",
    "# Initialize the Hill Climbing search\n",
    "model_hc = BayesianNetwork()\n",
    "hc = HillClimbSearch(pd.DataFrame(data_train_encoded))\n",
    "\n",
    "# Structure learning with Hill Climbing search algorithm\n",
    "best_model = hc.estimate(scoring_method='k2score')\n",
    "\n",
    "model_hc.add_nodes_from(best_model.nodes())\n",
    "model_hc.add_edges_from(best_model.edges())\n",
    "print(\"Nodes and edges of the HC model:\")\n",
    "print(model_hc.nodes())\n",
    "print(model_hc.edges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning with Maximum Likelihood Estimation, other methods are available\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "\n",
    "# Estimate parameters using Maximum Likelihood Estimation (MLE) for PC model\n",
    "model_pc.fit(pd.DataFrame(data_train_encoded), estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "# Estimate parameters using Maximum Likelihood Estimation (MLE) for HC model\n",
    "model_hc.fit(pd.DataFrame(data_train_encoded), estimator=MaximumLikelihoodEstimator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [00:02<00:00, 131.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions using PC\n",
      "      24   21   22   23\n",
      "0    0.0  0.0  0.0  1.0\n",
      "1    0.0  0.0  0.0  1.0\n",
      "2    0.0  0.0  0.0  1.0\n",
      "3    0.0  0.0  0.0  1.0\n",
      "4    0.0  0.0  0.0  1.0\n",
      "..   ...  ...  ...  ...\n",
      "341  0.0  0.0  0.0  1.0\n",
      "342  0.0  0.0  0.0  1.0\n",
      "343  0.0  0.0  0.0  1.0\n",
      "344  0.0  0.0  0.0  1.0\n",
      "345  0.0  0.0  0.0  1.0\n",
      "\n",
      "[346 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [00:00<00:00, 2369.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions using HC\n",
      "      24   21   22   23\n",
      "0    0.0  0.0  0.0  1.0\n",
      "1    0.0  1.0  0.0  0.0\n",
      "2    0.0  0.0  0.0  1.0\n",
      "3    0.0  1.0  0.0  0.0\n",
      "4    0.0  0.0  0.0  1.0\n",
      "..   ...  ...  ...  ...\n",
      "341  0.0  0.0  0.0  1.0\n",
      "342  0.0  0.0  0.0  1.0\n",
      "343  0.0  0.0  0.0  1.0\n",
      "344  0.0  0.0  0.0  1.0\n",
      "345  0.0  0.0  0.0  1.0\n",
      "\n",
      "[346 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Predictions using PC model\n",
    "y_pred_pc = model_pc.predict(pd.DataFrame(X_test))\n",
    "\n",
    "print(\"Predictions using PC\")\n",
    "print(y_pred_pc)\n",
    "\n",
    "# Predictions using HC model\n",
    "y_pred_hc = model_hc.predict(pd.DataFrame(X_test))\n",
    "\n",
    "print(\"Predictions using HC\")\n",
    "print(y_pred_hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes in PC model: [5, 4, 21, 22, 18, 19, 20, 14, 12, 6, 7, 24, 23, 2, 0, 3, 13, 16, 17, 1, 8, 9, 15, 11, 10]\n",
      "Edges in PC model: [(5, 4), (5, 7), (21, 22), (21, 23), (18, 19), (18, 24), (18, 20), (20, 24), (20, 19), (14, 12), (14, 13), (12, 23), (6, 4), (6, 7), (6, 5), (2, 0), (2, 3), (13, 12), (16, 17), (1, 0), (1, 3), (1, 2), (8, 9), (8, 10), (8, 11), (15, 17), (15, 16), (11, 9), (11, 10), (10, 9)]\n",
      "\n",
      "\n",
      "Nodes in HC model: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "Edges in HC model: [(0, 2), (0, 1), (0, 12), (1, 2), (3, 2), (3, 1), (3, 0), (3, 12), (4, 3), (5, 4), (5, 6), (6, 4), (7, 4), (7, 6), (7, 3), (7, 5), (9, 8), (9, 10), (9, 11), (10, 8), (11, 8), (11, 10), (12, 13), (12, 14), (14, 13), (15, 17), (16, 17), (16, 15), (17, 21), (18, 19), (18, 20), (18, 21), (19, 21), (19, 12), (19, 22), (19, 7), (20, 19), (21, 23), (21, 3), (21, 7), (22, 23), (22, 5), (22, 21), (22, 0), (22, 6), (22, 1), (23, 12), (23, 3), (23, 7), (24, 23), (24, 18), (24, 21), (24, 15), (24, 0), (24, 5)]\n"
     ]
    }
   ],
   "source": [
    "# Get the names of nodes (variables) in PC model\n",
    "print(\"Nodes in PC model:\", model_pc.nodes())\n",
    "print(\"Edges in PC model:\", model_pc.edges())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get the names of nodes (variables) in HC model\n",
    "print(\"Nodes in HC model:\", model_hc.nodes())\n",
    "print(\"Edges in HC model:\", model_hc.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure scores of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure score for PC:\n",
      "k2 score: -3384.823199700444\n",
      "bdeu score: -3424.808248803261\n",
      "bic score: -3375.2576757455663\n",
      "bds score: -3781.977868765447\n",
      "\n",
      "\n",
      "Structure score for HC:\n",
      "k2 score: -2957.889126228843\n",
      "bdeu score: -2958.594353603475\n",
      "bic score: -3152.0380245178853\n",
      "bds score: -3494.292343403773\n"
     ]
    }
   ],
   "source": [
    "from scores import compute_structure_score\n",
    "\n",
    "print(\"Structure score for PC:\")\n",
    "compute_structure_score(model_pc, data_test_encoded)\n",
    "print(\"\\n\")\n",
    "print(\"Structure score for HC:\")\n",
    "compute_structure_score(model_hc, data_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation scores of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation score for PC:\n",
      "chi_square score (accuracy): 0.9314516129032258\n",
      "g_sq score (accuracy): 0.9249492900608519\n",
      "log_likelihood score (accuracy): 0.9249492900608519\n",
      "freeman_tuckey score (accuracy): 0.9224489795918367\n",
      "modified_log_likelihood score (accuracy): 0.9202453987730062\n",
      "neyman score (accuracy): 0.9202453987730062\n",
      "cressie_read score (accuracy): 0.9292929292929293\n",
      "\n",
      "\n",
      "Correlation score for HC:\n",
      "chi_square score (accuracy): 0.5482866043613707\n",
      "g_sq score (accuracy): 0.5534591194968553\n",
      "log_likelihood score (accuracy): 0.5534591194968553\n",
      "freeman_tuckey score (accuracy): 0.5523809523809524\n",
      "modified_log_likelihood score (accuracy): 0.554140127388535\n",
      "neyman score (accuracy): 0.554140127388535\n",
      "cressie_read score (accuracy): 0.55\n"
     ]
    }
   ],
   "source": [
    "from scores import compute_correlation_score\n",
    "print(\"Correlation score for PC:\")\n",
    "compute_correlation_score(model_pc, data_test_encoded)\n",
    "print(\"\\n\")\n",
    "print(\"Correlation score for HC:\")\n",
    "compute_correlation_score(model_hc, data_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ispr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
