{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm 2 - Assignment 5\n",
    "\n",
    "## Main objective\n",
    "\n",
    "Learn the structure of the Bayesian Network (BN) resulting from the dataset (https://archive.ics.uci.edu/dataset/19/car+evaluation) using two BN structure learning algorithms of your choice.  For instance you can consider the algorithms implemented in PGMPY or any other comparable library (e.g. see the list of libraries listed in Lecture 7).  Compare and discuss the results obtained with the two different algorithms. Also discuss any hyperparameter/design choice you had to take.\n",
    "\n",
    "### Choices made\n",
    "\n",
    "For this assignment I chose the PC algorithm and the Hill Climbing algorithm to learn the structure of the Bayesian Network. The PC algorithm is a constraint-based algorithm that uses conditional independence tests to learn the structure of the network. The Hill Climbing algorithm is a score-based algorithm that uses a scoring function to evaluate the quality of the network structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "From the dataset I applied One-hot encoding to the categorical features, since we have more than 2 categories for each feature. I also split the dataset into training and testing sets, with 80% of the data used for training and 20% for testing.\n",
    "\n",
    "For completeness, I made possible to use the raw data, the data One-hot encoded with sklearn and the data One-hot encoded with pandas. The user can comment and uncomment the relative lines of code that I specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  buying  maint doors persons lug_boot safety  class\n",
      "0  vhigh  vhigh     2       2    small    low  unacc\n",
      "1  vhigh  vhigh     2       2    small    med  unacc\n",
      "2  vhigh  vhigh     2       2    small   high  unacc\n",
      "3  vhigh  vhigh     2       2      med    low  unacc\n",
      "4  vhigh  vhigh     2       2      med    med  unacc\n",
      "buying      0\n",
      "maint       0\n",
      "doors       0\n",
      "persons     0\n",
      "lug_boot    0\n",
      "safety      0\n",
      "class       0\n",
      "dtype: int64\n",
      "TR data\n",
      "      buying_high  buying_low  buying_med  buying_vhigh  maint_high  \\\n",
      "107         False       False       False          True       False   \n",
      "901         False       False        True         False       False   \n",
      "1709        False        True       False         False       False   \n",
      "706          True       False       False         False       False   \n",
      "678          True       False       False         False       False   \n",
      "...           ...         ...         ...           ...         ...   \n",
      "1130        False       False        True         False       False   \n",
      "1294        False       False        True         False       False   \n",
      "860          True       False       False         False       False   \n",
      "1459        False        True       False         False        True   \n",
      "1126        False       False        True         False       False   \n",
      "\n",
      "      maint_low  maint_med  maint_vhigh  doors_2  doors_3  ...  lug_boot_big  \\\n",
      "107       False      False         True    False    False  ...          True   \n",
      "901       False      False         True    False     True  ...         False   \n",
      "1709       True      False        False    False    False  ...          True   \n",
      "706       False       True        False    False    False  ...         False   \n",
      "678       False       True        False    False     True  ...         False   \n",
      "...         ...        ...          ...      ...      ...  ...           ...   \n",
      "1130      False       True        False    False     True  ...         False   \n",
      "1294       True      False        False    False    False  ...          True   \n",
      "860        True      False        False    False    False  ...         False   \n",
      "1459      False      False        False    False    False  ...         False   \n",
      "1126      False       True        False    False     True  ...         False   \n",
      "\n",
      "      lug_boot_med  lug_boot_small  safety_high  safety_low  safety_med  \\\n",
      "107          False           False         True       False       False   \n",
      "901          False            True        False       False        True   \n",
      "1709         False           False         True       False       False   \n",
      "706           True           False        False       False        True   \n",
      "678           True           False        False        True       False   \n",
      "...            ...             ...          ...         ...         ...   \n",
      "1130          True           False         True       False       False   \n",
      "1294         False           False        False       False        True   \n",
      "860           True           False         True       False       False   \n",
      "1459         False            True        False       False        True   \n",
      "1126         False            True        False       False        True   \n",
      "\n",
      "        acc   good  unacc  vgood  \n",
      "107   False  False   True  False  \n",
      "901   False  False   True  False  \n",
      "1709  False  False   True  False  \n",
      "706   False  False   True  False  \n",
      "678   False  False   True  False  \n",
      "...     ...    ...    ...    ...  \n",
      "1130  False  False  False   True  \n",
      "1294  False   True  False  False  \n",
      "860    True  False  False  False  \n",
      "1459  False  False   True  False  \n",
      "1126   True  False  False  False  \n",
      "\n",
      "[1382 rows x 25 columns]\n",
      "\n",
      "\n",
      "TS data\n",
      "      buying_high  buying_low  buying_med  buying_vhigh  maint_high  \\\n",
      "599          True       False       False         False        True   \n",
      "1201        False       False        True         False       False   \n",
      "628          True       False       False         False        True   \n",
      "1498        False        True       False         False        True   \n",
      "1263        False       False        True         False       False   \n",
      "...           ...         ...         ...           ...         ...   \n",
      "100         False       False       False          True       False   \n",
      "274         False       False       False          True       False   \n",
      "1206        False       False        True         False       False   \n",
      "101         False       False       False          True       False   \n",
      "1084        False       False        True         False       False   \n",
      "\n",
      "      maint_low  maint_med  maint_vhigh  doors_2  doors_3  ...  lug_boot_big  \\\n",
      "599       False      False        False    False    False  ...         False   \n",
      "1201       True      False        False     True    False  ...         False   \n",
      "628       False      False        False    False    False  ...          True   \n",
      "1498      False      False        False    False    False  ...         False   \n",
      "1263       True      False        False    False    False  ...         False   \n",
      "...         ...        ...          ...      ...      ...  ...           ...   \n",
      "100       False      False         True    False    False  ...         False   \n",
      "274       False       True        False    False    False  ...         False   \n",
      "1206       True      False        False     True    False  ...         False   \n",
      "101       False      False         True    False    False  ...         False   \n",
      "1084      False       True        False     True    False  ...         False   \n",
      "\n",
      "      lug_boot_med  lug_boot_small  safety_high  safety_low  safety_med  \\\n",
      "599           True           False         True       False       False   \n",
      "1201          True           False        False       False        True   \n",
      "628          False           False        False       False        True   \n",
      "1498          True           False        False       False        True   \n",
      "1263          True           False        False        True       False   \n",
      "...            ...             ...          ...         ...         ...   \n",
      "100          False            True        False       False        True   \n",
      "274           True           False        False       False        True   \n",
      "1206         False            True        False        True       False   \n",
      "101          False            True         True       False       False   \n",
      "1084          True           False        False       False        True   \n",
      "\n",
      "        acc   good  unacc  vgood  \n",
      "599   False  False   True  False  \n",
      "1201   True  False  False  False  \n",
      "628   False  False   True  False  \n",
      "1498   True  False  False  False  \n",
      "1263  False  False   True  False  \n",
      "...     ...    ...    ...    ...  \n",
      "100   False  False   True  False  \n",
      "274   False  False   True  False  \n",
      "1206  False  False   True  False  \n",
      "101   False  False   True  False  \n",
      "1084  False  False   True  False  \n",
      "\n",
      "[346 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\"\n",
    "names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
    "data = pd.read_csv(url, names=names)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Preprocessing\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Assuming 'data' is the original dataset containing features and target variable\n",
    "# Extract features (X) and target variable (y)\n",
    "X = data.drop(columns=['class'])  # Features\n",
    "y = data['class']  # Target variable\n",
    "\n",
    "# One-hot encode the features and target variable, with 0 and 1 like sklearn does\n",
    "# to use them, uncomment the lines that use np.append and comment the lines that use pd.concat\n",
    "\n",
    "#X_encoded = OneHotEncoder().fit_transform(X).toarray()\n",
    "#y_encoded = OneHotEncoder().fit_transform(y.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Data encoded with pandas, uncomment the following lines and the next commented lines that use pd.concat\n",
    "X_encoded = pd.get_dummies(X)\n",
    "y_encoded = pd.get_dummies(y)\n",
    "\n",
    "# Concatenate the features and target variable\n",
    "#data_encoded = np.append(X_encoded, y_encoded, axis=1)\n",
    "data_encoded = pd.concat([X_encoded, y_encoded], axis=1)\n",
    "\n",
    "# Split the dataset into training and test sets, uncomment the second line to use raw data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Unencoded data\n",
    "\n",
    "# Encoded data sets for training and testing\n",
    "data_train_encoded = np.append(X_train, y_train, axis=1)\n",
    "data_test_encoded = np.append(X_test, y_test, axis=1)\n",
    "\n",
    "# Unencoded data sets for training and testing\n",
    "#data_train_unencoded = pd.concat([X_train, y_train], axis=1)\n",
    "#data_test_unencoded = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "data_train_encoded = pd.concat([X_train, y_train], axis=1)\n",
    "data_test_encoded = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "print(\"TR data\")\n",
    "print(data_train_encoded)\n",
    "print(\"\\n\")\n",
    "print(\"TS data\")\n",
    "print(data_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PC algorithm\n",
    "\n",
    "In this section I have instantiate a Bayesian Network with the method of the library pgmpy and used the PC algorithm to learn the structure of the network. Then, I added the nodes and edges to the model.\n",
    "\n",
    "Initially, during testing I used the default variant of the PC algorithm, but since it was slow to compute the result, I used the 'parallel' variant, which is faster.\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "- variant: I chose the 'parallel' variant proposed by pgmpy because it exploits parallelism to speed up the algorithm.\n",
    "- significance_level: I chose the significance level of 0.01, the most common are 0.01 and 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielebedini/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Working for n conditional variables: 5: 100%|██████████| 5/5 [00:04<00:00,  1.84it/s]INFO:pgmpy:Reached maximum number of allowed conditional variables. Exiting\n",
      "Working for n conditional variables: 5: 100%|██████████| 5/5 [00:04<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes and edges of the PC model:\n",
      "['persons_more', 'persons_2', 'persons_4', 'maint_med', 'maint_vhigh', 'maint_low', 'lug_boot_med', 'lug_boot_small', 'maint_high', 'unacc', 'safety_med', 'vgood', 'safety_low', 'acc', 'good', 'buying_med', 'buying_vhigh', 'safety_high', 'buying_high', 'buying_low', 'doors_2', 'doors_3', 'doors_5more', 'lug_boot_big', 'doors_4']\n",
      "[('persons_more', 'persons_2'), ('persons_2', 'unacc'), ('persons_4', 'persons_2'), ('persons_4', 'persons_more'), ('maint_med', 'maint_high'), ('maint_med', 'maint_vhigh'), ('maint_med', 'maint_low'), ('maint_low', 'maint_high'), ('maint_low', 'maint_vhigh'), ('lug_boot_med', 'lug_boot_small'), ('safety_med', 'vgood'), ('safety_med', 'safety_low'), ('acc', 'unacc'), ('acc', 'good'), ('buying_med', 'buying_vhigh'), ('buying_med', 'buying_high'), ('safety_high', 'safety_low'), ('safety_high', 'vgood'), ('safety_high', 'safety_med'), ('buying_low', 'buying_vhigh'), ('buying_low', 'buying_high'), ('buying_low', 'buying_med'), ('doors_3', 'doors_2'), ('doors_5more', 'doors_2'), ('doors_5more', 'doors_3'), ('lug_boot_big', 'lug_boot_small'), ('lug_boot_big', 'lug_boot_med'), ('doors_4', 'doors_2'), ('doors_4', 'doors_3'), ('doors_4', 'doors_5more')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import PC\n",
    "from pgmpy.models import BayesianNetwork\n",
    "\n",
    "# Initialize the PC algorithm with data and a different significance level\n",
    "model_pc = BayesianNetwork()\n",
    "pc = PC(pd.DataFrame(data_train_encoded))\n",
    "\n",
    "# Constraint-based structure learning, with PC algorithm\n",
    "dag = pc.estimate(return_type='dag', variant='parallel', significance_level=0.01)\n",
    "\n",
    "model_pc.add_nodes_from(dag.nodes())    \n",
    "model_pc.add_edges_from(dag.edges())\n",
    "\n",
    "# Display the edges of the learned structure\n",
    "print(\"Nodes and edges of the PC model:\")\n",
    "print(model_pc.nodes())\n",
    "print(model_pc.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hill Climbing Algorithm\n",
    "\n",
    "In this section I have instantiate a Bayesian Network with the method of the library pgmpy and used the Hill Climbing algorithm to learn the structure of the network. Then, I added the nodes and edges to the model, as I did with the PC algorithm.\n",
    "\n",
    "The Hill Climbing algorithm is also computationally expensive, but it is faster than the PC algorithm. We can clearly see that the HC algorithm takes less than a second to complete. This fact can be nice and bad at the same time, because it can be stuck in a local maxima.\n",
    "\n",
    "#### Hyperparameters\n",
    "- scoring method: I chose the K2 score, which I tested with the dataset and it gave the best results, compared to the other proposed scores.\n",
    "\n",
    "- The other hyperparameters were kept as default, as I thought they were not important for the purpose of this assignment (I can be absolutely wrong)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 56/1000000 [00:00<4:30:10, 61.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes and edges of the HC model:\n",
      "['buying_high', 'buying_low', 'buying_med', 'buying_vhigh', 'maint_high', 'maint_low', 'maint_med', 'maint_vhigh', 'doors_2', 'doors_3', 'doors_4', 'doors_5more', 'persons_2', 'persons_4', 'persons_more', 'lug_boot_big', 'lug_boot_med', 'lug_boot_small', 'safety_high', 'safety_low', 'safety_med', 'acc', 'good', 'unacc', 'vgood']\n",
      "[('buying_high', 'buying_med'), ('buying_high', 'buying_low'), ('buying_high', 'persons_2'), ('buying_low', 'buying_med'), ('buying_vhigh', 'buying_med'), ('buying_vhigh', 'buying_low'), ('buying_vhigh', 'buying_high'), ('buying_vhigh', 'persons_2'), ('maint_high', 'buying_vhigh'), ('maint_low', 'maint_high'), ('maint_low', 'maint_med'), ('maint_med', 'maint_high'), ('maint_vhigh', 'maint_high'), ('maint_vhigh', 'maint_med'), ('maint_vhigh', 'buying_vhigh'), ('maint_vhigh', 'maint_low'), ('doors_3', 'doors_2'), ('doors_3', 'doors_4'), ('doors_3', 'doors_5more'), ('doors_4', 'doors_2'), ('doors_5more', 'doors_2'), ('doors_5more', 'doors_4'), ('persons_2', 'persons_4'), ('persons_2', 'persons_more'), ('persons_more', 'persons_4'), ('lug_boot_big', 'lug_boot_small'), ('lug_boot_med', 'lug_boot_small'), ('lug_boot_med', 'lug_boot_big'), ('lug_boot_small', 'acc'), ('safety_high', 'safety_low'), ('safety_high', 'safety_med'), ('safety_high', 'acc'), ('safety_low', 'acc'), ('safety_low', 'persons_2'), ('safety_low', 'good'), ('safety_low', 'maint_vhigh'), ('safety_med', 'safety_low'), ('acc', 'unacc'), ('acc', 'buying_vhigh'), ('acc', 'maint_vhigh'), ('good', 'unacc'), ('good', 'maint_low'), ('good', 'acc'), ('good', 'buying_high'), ('good', 'maint_med'), ('good', 'buying_low'), ('unacc', 'persons_2'), ('unacc', 'buying_vhigh'), ('unacc', 'maint_vhigh'), ('vgood', 'unacc'), ('vgood', 'safety_high'), ('vgood', 'acc'), ('vgood', 'lug_boot_big'), ('vgood', 'buying_high'), ('vgood', 'maint_low')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Learning with hill climbing\n",
    "from pgmpy.estimators import HillClimbSearch\n",
    "\n",
    "# Initialize the Hill Climbing search\n",
    "model_hc = BayesianNetwork()\n",
    "hc = HillClimbSearch(pd.DataFrame(data_train_encoded))\n",
    "\n",
    "# Structure learning with Hill Climbing search algorithm\n",
    "best_model = hc.estimate(scoring_method='k2score')\n",
    "\n",
    "model_hc.add_nodes_from(best_model.nodes())\n",
    "model_hc.add_edges_from(best_model.edges())\n",
    "print(\"Nodes and edges of the HC model:\")\n",
    "print(model_hc.nodes())\n",
    "print(model_hc.edges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Little sidenote\n",
    "\n",
    "I also tried and inserted the Max-min Hill Climbing search but it was taking too long to complete (when data is one-hot encoded), so I decided to remove it from the final version of the code. I left it commented in the code for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# MmHC algorithm\\nfrom pgmpy.estimators import MmhcEstimator\\n\\n# Initialize the MmHC algorithm\\nmodel_mmhc = BayesianNetwork()\\nmmhc = MmhcEstimator(pd.DataFrame(data_train_encoded))\\n\\n# Structure learning with MmHC algorithm\\nbest_model = mmhc.estimate(scoring_method=\\'k2score\\', significance_level=0.01)\\n\\nmodel_mmhc.add_nodes_from(best_model.nodes())\\nmodel_mmhc.add_edges_from(best_model.edges())\\nprint(\"Nodes and edges of the MMHC model:\")\\nprint(model_mmhc.nodes())\\nprint(model_mmhc.edges())\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# MmHC algorithm\n",
    "from pgmpy.estimators import MmhcEstimator\n",
    "\n",
    "# Initialize the MmHC algorithm\n",
    "model_mmhc = BayesianNetwork()\n",
    "mmhc = MmhcEstimator(pd.DataFrame(data_train_encoded))\n",
    "\n",
    "# Structure learning with MmHC algorithm\n",
    "best_model = mmhc.estimate(scoring_method='k2score', significance_level=0.01)\n",
    "\n",
    "model_mmhc.add_nodes_from(best_model.nodes())\n",
    "model_mmhc.add_edges_from(best_model.edges())\n",
    "print(\"Nodes and edges of the MMHC model:\")\n",
    "print(model_mmhc.nodes())\n",
    "print(model_mmhc.edges())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data fitting\n",
    "\n",
    "After learning the structure with the 2 methods, I fitted the training data into the models using the Maximum Likelihood Estimation (MLE) method for both of them.\n",
    "\n",
    "Other estimator can be used, such as the Bayesian Estimator, but for this assignment I chose the MLE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning with Maximum Likelihood Estimation, other methods are available\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "\n",
    "# Estimate parameters using Maximum Likelihood Estimation (MLE) for PC model\n",
    "model_pc.fit(pd.DataFrame(data_train_encoded), estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "# Estimate parameters using Maximum Likelihood Estimation (MLE) for HC model\n",
    "model_hc.fit(pd.DataFrame(data_train_encoded), estimator=MaximumLikelihoodEstimator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions \n",
    "\n",
    "After fitting the data into the models, I made predictions using the testing data. I used the predict() method to predict the values of the target variable given the values of the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [00:00<00:00, 3016.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions using PC\n",
      "     vgood  unacc    acc   good\n",
      "0    False   True  False  False\n",
      "1    False   True  False  False\n",
      "2    False   True  False  False\n",
      "3    False   True  False  False\n",
      "4    False   True  False  False\n",
      "..     ...    ...    ...    ...\n",
      "341  False   True  False  False\n",
      "342  False   True  False  False\n",
      "343  False   True  False  False\n",
      "344  False   True  False  False\n",
      "345  False   True  False  False\n",
      "\n",
      "[346 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [00:00<00:00, 2863.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions using HC\n",
      "     vgood  unacc    acc   good\n",
      "0    False   True  False  False\n",
      "1    False  False   True  False\n",
      "2    False   True  False  False\n",
      "3    False  False   True  False\n",
      "4    False   True  False  False\n",
      "..     ...    ...    ...    ...\n",
      "341  False   True  False  False\n",
      "342  False   True  False  False\n",
      "343  False   True  False  False\n",
      "344  False   True  False  False\n",
      "345  False   True  False  False\n",
      "\n",
      "[346 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Predictions using PC model\n",
    "y_pred_pc = model_pc.predict(pd.DataFrame(X_test))\n",
    "\n",
    "print(\"Predictions using PC\")\n",
    "print(y_pred_pc)\n",
    "\n",
    "# Predictions using HC model\n",
    "y_pred_hc = model_hc.predict(pd.DataFrame(X_test))\n",
    "\n",
    "print(\"Predictions using HC\")\n",
    "print(y_pred_hc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network dimensions\n",
    "\n",
    "After training and testing the models I printed the number of nodes and edges of the networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes in PC model: 25\n",
      "Total edges in PC model: 30\n",
      "\n",
      "\n",
      "Total nodes in HC model: 25\n",
      "Total edges in HC model: 55\n"
     ]
    }
   ],
   "source": [
    "# Get the number of nodes (variables) and edges in PC model\n",
    "print(\"Total nodes in PC model:\", len(model_pc.nodes()))\n",
    "print(\"Total edges in PC model:\", len(model_pc.edges()))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get the number of nodes (variables) and edges in HC model\n",
    "print(\"Total nodes in HC model:\", len(model_hc.nodes()))\n",
    "print(\"Total edges in HC model:\", len(model_hc.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final scores\n",
    "\n",
    "After training and testing, I calculated 2 kinds of scoring methods used in the pgmpy library: structure and correlation scores. In particular, these scores are used to evaluate the quality of the structure of the Bayesian Network.\n",
    "\n",
    "In scores.py I implemented the 2 methods that print all the possible structure and correlation scores for the models, in order to better evaluate the quality of the structure of the Bayesian Networks and compare them.\n",
    "\n",
    "### Structure scores of the models\n",
    "\n",
    "Structure scores in Bayesian networks are measures used to evaluate the goodness of fit of a Bayesian network structure to data. These scores are used in score-based learning algorithms, which aim to find the structure that maximizes the score given the data (like Hill Climbing and variants do).\n",
    "\n",
    "As we can see from the results, the Hill Climbing algorithm has a slightly higher structure score (less negative) than the PC algorithm. This means that the structure learned by the Hill Climbing algorithm is a better fit to the data than the structure learned by the PC algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure scores for PC:\n",
      "k2 score: -3384.79765440193\n",
      "bdeu score: -3424.8082488032614\n",
      "bic score: -3375.257675745566\n",
      "bds score: -3781.992503433748\n",
      "\n",
      "\n",
      "Structure scores for HC:\n",
      "k2 score: -2957.889126228843\n",
      "bdeu score: -2958.594353603475\n",
      "bic score: -3152.0380245178853\n",
      "bds score: -3494.292343403773\n"
     ]
    }
   ],
   "source": [
    "from scores import compute_structure_score\n",
    "\n",
    "print(\"Structure scores for PC:\")\n",
    "compute_structure_score(model_pc, data_test_encoded)\n",
    "print(\"\\n\")\n",
    "print(\"Structure scores for HC:\")\n",
    "compute_structure_score(model_hc, data_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation scores of the models\n",
    "\n",
    "Correlation scores in Bayesian networks refer to measures of the strength and direction of the relationships between variables in a Bayesian network. These scores can be used to evaluate the goodness of fit of a Bayesian network to data, or to compare different Bayesian network structures.\n",
    "\n",
    "As we can see from the results, the model learned by the PC algorithm has high correlation scores, which means that the relationships between variables in the model are strong and well-defined. The model learned by the Hill Climbing algorithm has lower correlation scores, which means that the relationships between variables in the model are weaker and less well-defined.\n",
    "\n",
    "This is due to the fact that the Hill Climbing algorithm is a greedy algorithm that may not find the optimal structure.\n",
    "In the other hand, the PC algorithm has a non-greedy nature, which may result in a more accurate structure (like in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation scores for PC:\n",
      "chi_square score (accuracy): 0.9314516129032258\n",
      "g_sq score (accuracy): 0.9249492900608519\n",
      "log_likelihood score (accuracy): 0.9249492900608519\n",
      "freeman_tuckey score (accuracy): 0.9224489795918367\n",
      "modified_log_likelihood score (accuracy): 0.9202453987730062\n",
      "neyman score (accuracy): 0.9202453987730062\n",
      "cressie_read score (accuracy): 0.9292929292929293\n",
      "\n",
      "\n",
      "Correlation scores for HC:\n",
      "chi_square score (accuracy): 0.5482866043613707\n",
      "g_sq score (accuracy): 0.5534591194968553\n",
      "log_likelihood score (accuracy): 0.5534591194968553\n",
      "freeman_tuckey score (accuracy): 0.5523809523809524\n",
      "modified_log_likelihood score (accuracy): 0.554140127388535\n",
      "neyman score (accuracy): 0.554140127388535\n",
      "cressie_read score (accuracy): 0.55\n"
     ]
    }
   ],
   "source": [
    "from scores import compute_correlation_score\n",
    "print(\"Correlation scores for PC:\")\n",
    "compute_correlation_score(model_pc, data_test_encoded)\n",
    "print(\"\\n\")\n",
    "print(\"Correlation scores for HC:\")\n",
    "compute_correlation_score(model_hc, data_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final considerations\n",
    "\n",
    "We observed that the 2 algorithms are very different and have different strengths and weaknesses. The PC algorithm is computationally expensive but may result in a more accurate structure, while the Hill Climbing algorithm is faster but may not find the optimal structure, as it can get stuck in local maxima. There are also other variants of the Hill Climbing algorithm that may perform better, such as the Max-min Hill Climbing search, but for this assignment I chose the basic Hill Climbing algorithm.\n",
    "\n",
    "The PC algorithm resulted in a better structure than the Hill Climbing algorithm, as we can see from the structure and correlation scores. However, this may not always be the case, as the performance of the algorithms may depend on the dataset and the specific problem being solved.\n",
    "\n",
    "Depending on the problem, it may be necessary to try different algorithms and hyperparameters to find the best structure for the Bayesian Network. In this case, the PC algorithm was the best choice, but in other cases the Hill Climbing algorithm or other algorithms may perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This assignment is limited to the dataset used and the algorithms implemented. The results may vary with different datasets and algorithms. However, the results obtained in this assignment are consistent with what is expected from the PC and Hill Climbing algorithms, based on their characteristics and performance.\n",
    "\n",
    "Also, only the default versions of the PC and HC algorithms were used, in literature there are many variants of these algorithms that can be used to improve the results.\n",
    "\n",
    "As coded in the file \"try_plot.py\" I tried to plot the Bayesian Networks, but was only for my personal curiosity, so I decided to not insert it in the final report because it was not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal considerations\n",
    "\n",
    "By doing this assignment I learned a lot about Bayesian Networks and the algorithms used to learn their structure. It would be interesting in the future to try other variants or other algorithms and hyperparameters to see how they perform on this dataset.\n",
    "\n",
    "This project is also on GitHub. At this link you can find the complete code: https://github.com/danielebedini/ISPR_assignment2.git"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ispr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
