{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm 2 - Assignment 5\n",
    "\n",
    "### Main objective\n",
    "\n",
    "Learn the structure of the Bayesian Network (BN) resulting from the dataset (https://archive.ics.uci.edu/dataset/19/car+evaluation) using two BN structure learning algorithms of your choice.  For instance you can consider the algorithms implemented in PGMPY or any other comparable library (e.g. see the list of libraries listed in Lecture 7).  Compare and discuss the results obtained with the two different algorithms. Also discuss any hyperparameter/design choice you had to take.\n",
    "\n",
    "### Choices made\n",
    "\n",
    "For this assignment I chose the PC algorithm and the Hill Climbing algorithm to learn the structure of the Bayesian Network. The PC algorithm is a constraint-based algorithm that uses conditional independence tests to learn the structure of the network. The Hill Climbing algorithm is a score-based algorithm that uses a scoring function to evaluate the quality of the network structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "From the dataset I applied One-hot encoding to the categorical features, since we have more than 2 categories for each feature. I also split the dataset into training and testing sets, with 80% of the data used for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  buying  maint doors persons lug_boot safety  class\n",
      "0  vhigh  vhigh     2       2    small    low  unacc\n",
      "1  vhigh  vhigh     2       2    small    med  unacc\n",
      "2  vhigh  vhigh     2       2    small   high  unacc\n",
      "3  vhigh  vhigh     2       2      med    low  unacc\n",
      "4  vhigh  vhigh     2       2      med    med  unacc\n",
      "TR data\n",
      "[[0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 1. 0.]\n",
      " [0. 1. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n",
      "\n",
      "\n",
      "TS data\n",
      "[[1. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\"\n",
    "names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
    "data = pd.read_csv(url, names=names)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Preprocessing\n",
    "# Check for missing values\n",
    "# print(data.isnull().sum())\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming 'data' is the original dataset containing features and target variable\n",
    "# Extract features (X) and target variable (y)\n",
    "X = data.drop(columns=['class'])  # Features\n",
    "y = data['class']  # Target variable\n",
    "\n",
    "# One-hot encode the features and target variable\n",
    "X_encoded = OneHotEncoder().fit_transform(X).toarray()\n",
    "y_encoded = OneHotEncoder().fit_transform(y.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Concatenate the features and target variable\n",
    "data_encoded = np.append(X_encoded, y_encoded, axis=1)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "data_train_encoded = np.append(X_train, y_train, axis=1)\n",
    "data_test_encoded = np.append(X_test, y_test, axis=1)\n",
    "\n",
    "print(\"TR data\")\n",
    "print(data_train_encoded)\n",
    "print(\"\\n\")\n",
    "print(\"TS data\")\n",
    "print(data_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PC algorithm\n",
    "\n",
    "In this section I have instantiate a Bayesian Network with the method of the library pgmpy and used the PC algorithm to learn the structure of the network. Then, I added the nodes and edges to the model.\n",
    "\n",
    "Here, we can observe that the algorithm takes quite a while to complete, which is expected since the PC algorithm is known to be computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working for n conditional variables: 5: 100%|██████████| 5/5 [00:04<00:00,  1.02s/it]INFO:pgmpy:Reached maximum number of allowed conditional variables. Exiting\n",
      "Working for n conditional variables: 5: 100%|██████████| 5/5 [00:05<00:00,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes and edges of the PC model:\n",
      "[5, 4, 21, 22, 18, 19, 20, 14, 12, 6, 7, 24, 23, 2, 0, 3, 13, 16, 17, 1, 8, 9, 15, 11, 10]\n",
      "[(5, 4), (5, 7), (21, 22), (21, 23), (18, 19), (18, 24), (18, 20), (20, 24), (20, 19), (14, 12), (14, 13), (12, 23), (6, 4), (6, 7), (6, 5), (2, 0), (2, 3), (13, 12), (16, 17), (1, 0), (1, 3), (1, 2), (8, 9), (8, 10), (8, 11), (15, 17), (15, 16), (11, 9), (11, 10), (10, 9)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import PC\n",
    "from pgmpy.models import BayesianNetwork\n",
    "\n",
    "# Initialize the PC algorithm with data and a different significance level\n",
    "model_pc = BayesianNetwork()\n",
    "pc = PC(pd.DataFrame(data_train_encoded))\n",
    "\n",
    "# Constraint-based structure learning, with PC algorithm\n",
    "dag = pc.estimate(return_type='dag')\n",
    "\n",
    "model_pc.add_nodes_from(dag.nodes())    \n",
    "model_pc.add_edges_from(dag.edges())\n",
    "\n",
    "# Display the edges of the learned structure\n",
    "print(\"Nodes and edges of the PC model:\")\n",
    "print(model_pc.nodes())\n",
    "print(model_pc.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hill Climbing Algorithm\n",
    "\n",
    "In this section I have instantiate a Bayesian Network with the method of the library pgmpy and used the Hill Climbing algorithm to learn the structure of the network. Then, I added the nodes and edges to the model, as I did with the PC algorithm.\n",
    "\n",
    "The Hill Climbing algorithm is also computationally expensive, but it is faster than the PC algorithm. We can clearly see that the HC algorithm takes less than a second to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 56/1000000 [00:00<4:39:39, 59.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes and edges of the HC model:\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "[(0, 2), (0, 1), (0, 12), (1, 2), (3, 2), (3, 1), (3, 0), (3, 12), (4, 3), (5, 4), (5, 6), (6, 4), (7, 4), (7, 6), (7, 3), (7, 5), (9, 8), (9, 10), (9, 11), (10, 8), (11, 8), (11, 10), (12, 13), (12, 14), (14, 13), (15, 17), (16, 17), (16, 15), (17, 21), (18, 19), (18, 20), (18, 21), (19, 21), (19, 12), (19, 22), (19, 7), (20, 19), (21, 23), (21, 3), (21, 7), (22, 23), (22, 5), (22, 21), (22, 0), (22, 6), (22, 1), (23, 12), (23, 3), (23, 7), (24, 23), (24, 18), (24, 21), (24, 15), (24, 0), (24, 5)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Learning with hill climbing\n",
    "from pgmpy.estimators import HillClimbSearch\n",
    "\n",
    "# Initialize the Hill Climbing search\n",
    "model_hc = BayesianNetwork()\n",
    "hc = HillClimbSearch(pd.DataFrame(data_train_encoded))\n",
    "\n",
    "# Structure learning with Hill Climbing search algorithm\n",
    "best_model = hc.estimate(scoring_method='k2score')\n",
    "\n",
    "model_hc.add_nodes_from(best_model.nodes())\n",
    "model_hc.add_edges_from(best_model.edges())\n",
    "print(\"Nodes and edges of the HC model:\")\n",
    "print(model_hc.nodes())\n",
    "print(model_hc.edges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data fitting\n",
    "\n",
    "After learning the structure with the 2 methods, I fitted the training data into the models using the Maximum Likelihood Estimation (MLE) method for both of them.\n",
    "\n",
    "Other estimator can be used, such as the Bayesian Estimator, but for this assignment I chose the MLE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning with Maximum Likelihood Estimation, other methods are available\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "\n",
    "# Estimate parameters using Maximum Likelihood Estimation (MLE) for PC model\n",
    "model_pc.fit(pd.DataFrame(data_train_encoded), estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "# Estimate parameters using Maximum Likelihood Estimation (MLE) for HC model\n",
    "model_hc.fit(pd.DataFrame(data_train_encoded), estimator=MaximumLikelihoodEstimator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions \n",
    "\n",
    "After fitting the data into the models, I made predictions using the testing data. I used the predict() method to predict the values of the target variable given the values of the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [00:02<00:00, 127.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions using PC\n",
      "      24   21   22   23\n",
      "0    0.0  0.0  0.0  1.0\n",
      "1    0.0  0.0  0.0  1.0\n",
      "2    0.0  0.0  0.0  1.0\n",
      "3    0.0  0.0  0.0  1.0\n",
      "4    0.0  0.0  0.0  1.0\n",
      "..   ...  ...  ...  ...\n",
      "341  0.0  0.0  0.0  1.0\n",
      "342  0.0  0.0  0.0  1.0\n",
      "343  0.0  0.0  0.0  1.0\n",
      "344  0.0  0.0  0.0  1.0\n",
      "345  0.0  0.0  0.0  1.0\n",
      "\n",
      "[346 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [00:00<00:00, 2048.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions using HC\n",
      "      24   21   22   23\n",
      "0    0.0  0.0  0.0  1.0\n",
      "1    0.0  1.0  0.0  0.0\n",
      "2    0.0  0.0  0.0  1.0\n",
      "3    0.0  1.0  0.0  0.0\n",
      "4    0.0  0.0  0.0  1.0\n",
      "..   ...  ...  ...  ...\n",
      "341  0.0  0.0  0.0  1.0\n",
      "342  0.0  0.0  0.0  1.0\n",
      "343  0.0  0.0  0.0  1.0\n",
      "344  0.0  0.0  0.0  1.0\n",
      "345  0.0  0.0  0.0  1.0\n",
      "\n",
      "[346 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Predictions using PC model\n",
    "y_pred_pc = model_pc.predict(pd.DataFrame(X_test))\n",
    "\n",
    "print(\"Predictions using PC\")\n",
    "print(y_pred_pc)\n",
    "\n",
    "# Predictions using HC model\n",
    "y_pred_hc = model_hc.predict(pd.DataFrame(X_test))\n",
    "\n",
    "print(\"Predictions using HC\")\n",
    "print(y_pred_hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes in PC model: 25\n",
      "Total edges in PC model: 30\n",
      "\n",
      "\n",
      "Total nodes in HC model: 25\n",
      "Total edges in HC model: 55\n"
     ]
    }
   ],
   "source": [
    "# Get the names of nodes (variables) in PC model\n",
    "print(\"Total nodes in PC model:\", len(model_pc.nodes()))\n",
    "print(\"Total edges in PC model:\", len(model_pc.edges()))\n",
    "#print(\"Nodes in PC model:\", model_pc.nodes())\n",
    "#print(\"Edges in PC model:\", model_pc.edges())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get the names of nodes (variables) in HC model\n",
    "#print(\"Nodes in HC model:\", model_hc.nodes())\n",
    "#print(\"Edges in HC model:\", model_hc.edges())\n",
    "print(\"Total nodes in HC model:\", len(model_hc.nodes()))\n",
    "print(\"Total edges in HC model:\", len(model_hc.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final scores\n",
    "\n",
    "After training and testing, I calculated 2 kinds of scoring methods used in the pgmpy library: structure and correlation scores. In particular, these scores are used to evaluate the quality of the structure of the Bayesian Network.\n",
    "\n",
    "### Structure scores of the models\n",
    "\n",
    "The structure scores are metrics that are used to evaluate the quality of the structure of the Bayesian Network. The higher the score, the better the structure of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure score for PC:\n",
      "k2 score: -3384.823199700444\n",
      "bdeu score: -3424.808248803261\n",
      "bic score: -3375.2576757455663\n",
      "bds score: -3781.977868765447\n",
      "\n",
      "\n",
      "Structure score for HC:\n",
      "k2 score: -2957.889126228843\n",
      "bdeu score: -2958.594353603475\n",
      "bic score: -3152.0380245178853\n",
      "bds score: -3494.292343403773\n"
     ]
    }
   ],
   "source": [
    "from scores import compute_structure_score\n",
    "\n",
    "print(\"Structure score for PC:\")\n",
    "compute_structure_score(model_pc, data_test_encoded)\n",
    "print(\"\\n\")\n",
    "print(\"Structure score for HC:\")\n",
    "compute_structure_score(model_hc, data_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation scores of the models\n",
    "\n",
    "Correlation scores of the Bayesian models are used to evaluate the quality of the model. The higher the score, the better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation score for PC:\n",
      "chi_square score (accuracy): 0.9314516129032258\n",
      "g_sq score (accuracy): 0.9249492900608519\n",
      "log_likelihood score (accuracy): 0.9249492900608519\n",
      "freeman_tuckey score (accuracy): 0.9224489795918367\n",
      "modified_log_likelihood score (accuracy): 0.9202453987730062\n",
      "neyman score (accuracy): 0.9202453987730062\n",
      "cressie_read score (accuracy): 0.9292929292929293\n",
      "\n",
      "\n",
      "Correlation score for HC:\n",
      "chi_square score (accuracy): 0.5482866043613707\n",
      "g_sq score (accuracy): 0.5534591194968553\n",
      "log_likelihood score (accuracy): 0.5534591194968553\n",
      "freeman_tuckey score (accuracy): 0.5523809523809524\n",
      "modified_log_likelihood score (accuracy): 0.554140127388535\n",
      "neyman score (accuracy): 0.554140127388535\n",
      "cressie_read score (accuracy): 0.55\n"
     ]
    }
   ],
   "source": [
    "from scores import compute_correlation_score\n",
    "print(\"Correlation score for PC:\")\n",
    "compute_correlation_score(model_pc, data_test_encoded)\n",
    "print(\"\\n\")\n",
    "print(\"Correlation score for HC:\")\n",
    "compute_correlation_score(model_hc, data_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ispr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
